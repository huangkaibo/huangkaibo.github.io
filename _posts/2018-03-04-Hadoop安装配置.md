---
title: Hadoop安装配置
date: 2018-03-04
tags: [High Performance]
---

内容: 介绍了Hadoop以及如何搭建

精华: 有不少报错解决方案

<!-- more -->

# 环境配置

教程版本为2.6
(只有一处命令需要版本号)

[官网教程](http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation)

配置为伪分布模式

```
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```
上面确实是9000
hadoop涉及好多个端口, 官网上的没错
[hadoop涉及的端口](http://blog.csdn.net/wulantian/article/details/46341043)

`$ bin/hdfs namenode -format`

这个不能执行太多次, 要不然data node会开不起来
要去hadoop目录下删除临时文件再重新`$ bin/hdfs namenode -format`

`sbin/start-dfs.sh`这个命令完成后就开启hadoop了
官网后面的教程是测试example, 不用管

`jps`命令看看要开的是不是都开启了

![](http://media.huangkaibo.cn/17-12-30/19807351.jpg)

# 配置环境变量

如果报错JAVA_HOME找不到
直接去hadoop的配置里修改JAVA_HOME

![](http://media.huangkaibo.cn/17-12-30/40718168.jpg)

加入hadoop的环境变量
```
# hadoop enviroment
# 只有第一第二个需要修改
export HADOOP_PREFIX=/usr/local/hadoop-2.9.0
export HADOOP_HOME=/usr/local/hadoop-2.9.0
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/lib
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH
```

# 运行

[hadoop shell命令大全](http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html)

## 编译代码

`javac Sort.java`

`jar -cvf Sort.jar *.class`

hadoop接收jar输入

先创建输入文件
以排序为例
创建data1  data2
都是存放随机数
放在input文件夹里

`hadoop dfs -put input`

将本地的input文件夹, 复制到hdfs文件系统里

`hadoop jar Sort.jar Sort input output`

无报错的话
hdfs文件系统里就有了output文件夹

![](http://media.huangkaibo.cn/17-12-30/28098800.jpg)

cat一下就能看到结果了

## 代码书写

[词频统计](http://blog.csdn.net/litianxiang_kaola/article/details/71154302)

### Mapper

![](http://media.huangkaibo.cn/17-12-30/6896171.jpg)

直接传出键值对
键为该行的数字转为字符
值为1

### Reducer

![](http://media.huangkaibo.cn/17-12-30/47900333.jpg)

取得maper返回的键值对, 直接输出

### main

![](http://media.huangkaibo.cn/17-12-30/61763524.jpg)

Main就是先初始化配置, 开启job
设置maper和reducer
然后设置maper和reducer的键值对
再设置输入输出文件

# 报错

```
root@VM-169-166-ubuntu:/home/workplace/mapreduce# hdfs dfs -ls
Java HotSpot(TM) Client VM warning: You have loaded library /usr/local/hadoop-2.9.0/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.
It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.
17/12/29 18:00:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
```

[参考](http://blog.csdn.net/l1028386804/article/details/51538611)

---

```
Error: JAVA_HOME is not set.
```

直接去hadoop的配置里修改JAVA_HOME

![](http://media.huangkaibo.cn/17-12-30/12510868.jpg)

---

![](http://media.huangkaibo.cn/17-12-30/26472741.jpg)

jar包找不到
需要环境变量的这条

`export CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH`

---

```
在执行./start-all.sh时，会卡在starting secondarynamenode

hadoop@zzl1:~/hadoopinstall/hadoop-1.1.2/bin$ ./start-all.sh starting namenode, logging to /home/hadoop/hadoopinstall/hadoop-1.1.2/libexec/../logs/hadoop-hadoop-namenode-zzl1.out
zzl2: starting datanode, logging to /home/hadoop/hadoopinstall/hadoop-1.1.2/libexec/../logs/hadoop-hadoop-datanode-zzl2.out
192.168.211.132: datanode running as process 8609. Stop it first.
hadoop@192.168.211.130's password: hadoop@zzl1's password: 
192.168.211.130: starting secondarynamenode, logging to /home/hadoop/hadoopinstall/hadoop-1.1.2/libexec/../logs/hadoop-hadoop-secondarynamenode-zzl1.out
```

datanode上次运行时没有关掉，所以先stop-all，再start-all
