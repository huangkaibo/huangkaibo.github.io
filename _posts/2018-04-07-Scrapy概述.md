---
title: Scrapy概述
date: 2018-04-07
tags: [python,spider]
---

内容: Scrapy常用组件介绍

精华: 非常用心的解释了每个常用组件的使用

<!-- more -->

# 组件概述

```
//demo是项目名, 可以随便改
//创建一个爬虫项目
scrapy startproject demo
```

![](http://p1rbtn7qp.bkt.clouddn.com/18-4-2/86925084.jpg)

* demo(项目文件夹)
    * demo(与项目同名)
        * \_pycache\_
        * spiders: 这里写爬虫
            * \_pycache\_
            * \_init\_.py
        * \_init\_.py
        * items.py: entity文件, dict形式使用
        * middlewares.py: 中间件/钩子, 爬取前后预处理(如修改header, url过滤)
        * pipelines.py: dao文件, 负责处理数据
        * settings.py: 爬虫详细配置文件, 以及自定义键值对配置(如数据库ip/数据库用户名)
    * scrapy.cfg: 项目配置文件

# 创建爬虫文件

```python
# spiders下新建py文件
import scrapy

class DemoSpider(scrapy.spiders.Spider):
    # 爬虫的名字
    name = "demoSpider"
    # 允许爬的域名(start_urls默认允许爬)
    # url遍历时来过滤
    allowed_domains = ["iplaysoft.com"]
    # 初始url
    start_urls = [
        "https://www.iplaysoft.com/"
    ]

    # 网址会自动访问, 这里对网址的Response进行解析处理
    # 每个start_urls会被分配一个Request对象进行访问
    # 并将parse方法作为回调函数传给Request
    def parse(self, response):
        filename = response.url.split("/")[-2]
        with open(filename, 'wb') as f:
            f.write(response.body)
```

# items.py

## 定义item

item就是entity文件

而且还不用指定类型

```
# 类型都写为scrapy.Field()
class ImageInfo(scrapy.Item):
    image_id = scrapy.Field()
    labels = scrapy.Field()
    image_urls = scrapy.Field()
    image = scrapy.Field()
```

## 使用item

```
# 一般在spider文件中使用
from 项目名.items import item名

items = []
for product in products_dict['data']['products']:
    item = PicInfo()
    item['pic_id'] = product['productId']
    item['labels'] = product['productName']
    items.append(item)
return items

# 也可以不用items, 生成一个item就直接yield, 也可以都处理
# 即使一次性return items, pipeline的process_item()一次也只会获取一个item
```

# pipelines.py

yield item之后该item会被pipeline管道捕获处理, 多个pipeline的话按顺序处理

## pipeline概述

pipeline类不须继承基础类, 只需在`settings.py`里注册

pipeline总共有1个必须实现和3个可以实现的接口

* **process_item(self, item, spider)**: 必须实现, spider捕获到item时自动调用此函数, 进行数据库操作
* open_spider(self, spider): spider开始时会调用这个函数, 一般进行数据库连接
* close_spider(self, spider): spider结束时会调用这个函数, 一般进行清理操作
* from_crawler(cls, crawler)

## 开启pipeline

```
# setting.py
# 将自己的pipeline添加进来
# item按照数字大小从小的piepline流到大的pipeline
ITEM_PIPELINES = {
   'vip.pipelines.DataStorePipeline': 300,
   'vip.pipelines.ImageDownloadPipeline': 200,
}
```

## pipeline顺序演示

```
# spider里的parse如下
def parse(self, response):
    for sel in response.xpath('//ul/li'):
        item = DemoItem()
        # new了item之后输出
        print("just new item")
        item['title'] = sel.xpath('a/text()').extract()
        item['link'] = sel.xpath('a/@href').extract()
        item['des'] = sel.xpath('text()').extract()
        # 即将yield之前输出
        print("about to yield item")
        yield item
        
# pipeline代码如下
class DemoPipeline(object):
    def process_item(self, item, spider):
        print('this is process_item()')
        return item
    def open_spider(self, spider):
        print("this is open_spider()")
        return
    def close_spider(self, spider):
        print("this is close_spider()")
        return
```

输出结果如下

![](http://p1rbtn7qp.bkt.clouddn.com/18-4-3/47549047.jpg)

在开始爬之前启动spider时调用了pipeline的open_spider()

![](http://p1rbtn7qp.bkt.clouddn.com/18-4-3/18801029.jpg)

爬到一个数据进入一次process_item(), 而且是yield之后才进入, 而非new时

![](http://p1rbtn7qp.bkt.clouddn.com/18-4-3/86882097.jpg)

数据爬完, 要关闭spider时进入close_spider()

## pipeline功能演示

```
class DataStorePipeline(object):
    # open_spider()一般用于创建数据库连接
    def open_spider(self, spider):
        """
        获取mysql连接,游标
        """
        self.conn = pymysql.connect(
            host = "localhost",
            user = "root",
            passwd = "root",
            db = "vip",
            charset="utf8",
            use_unicode=True
        )
        self.cursor = self.conn.cursor()
    
    # process_item()用于把item内容存进数据库
    def process_item(self, item, spider):
        """
        将item存储进数据库
        """
        sql = "INSERT INTO image_info(image_id, labels) VALUES('" + item['image_id'] + "', '" + item['labels'] + "');"
        try:
            self.cursor.execute(sql)
            self.conn.commit()
        except:
            self.conn.rollback()

    # close_spider()用于回收数据库连接
    def close_spider(self, spider):
        """
        关闭mysql连接,游标
        """
        self.cursor.close()
        self.conn.close()
```

# Request对象

```
# []代表可选
class scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback])
```

* callback: scrapy的request并不是马上发送， 而是yield Request后， 经由scrapy的Scheduler传给Downloader组件负责Request请求， 返回Response后传给回调函数， 如果没有指定回调函数， 默认调用parse()
* meta: 共享变量, 用于在Request里携带数据传递(见自定义图片名小结, 有使用到)

# 图片管道

```
# settings.py
# 第一个是开启系统的图片管道(只用开启就好, 不用实现)
# 第二个是自己定制的图片管道(自己实现)
ITEM_PIPELINES = {
   'scrapy.contrib.pipeline.images.ImagesPipeline': 1,
   'vip.pipelines.ImageDownloadPipeline': 200,
}

# 定义如下常量串, 为图片存储路径
# 命名不可以改, 系统调用的, 自己用不到
IMAGES_STORE = 'D:\\WorkPlace\\image-search\\images'
```

```
# item里添加两项
# image_urls和image, 这个命名是固定的, 不可以改
# image存的是图片信息, 图片校验码, 下载地址(从image_url里取得)什么的
class ImageInfo(scrapy.Item):
    image_id = scrapy.Field()
    labels = scrapy.Field()
    image_urls = scrapy.Field()
    image = scrapy.Field()
```

```
# 生产item时填充item['image_urls']
# item['image_urls']必须要是list类型, 所以前后加了[]
# item['image']不用管
item['image_urls'] = [product['smallImage'].strip()]
```

```
from scrapy.contrib.pipeline.images import ImagesPipeline
from scrapy.exceptions import DropItem

# 自己定制pipeline
# 这里除了类名都是官方给的代码, 不用修改
class ImageDownloadPipeline(ImagesPipeline):
    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            yield scrapy.Request(image_url, meta={'item': item})

    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
        if not image_paths:
            raise DropItem("Item contains no images")
        return item
```

然后就可以了

## 自定义图片名

### 利用os.rename()

```
def item_completed(self, results, item, info):
    image_paths = [x['path'] for ok, x in results if ok]
    if not image_paths:
        raise DropItem("Item contains no images")
    os.rename('D:/WorkPlace/image-search/images/' + image_paths[0], 'D:/WorkPlace/image-search/images/full/' + item['image_id'] + '.jpg')
    return item
```

### 重写图片管道的路径方法

```
# Request里多传一个参数meta, meta是共享变量, 这里携带了item
def get_media_requests(self, item, info):
    for image_url in item['image_urls']:
        yield scrapy.Request(image_url, meta={'item': item})

# 自定义的图片管道里添加如下方法
def file_path(self, request, response=None, info=None):
    # 从meta里获取item
    item = request.meta['item']
    return '/' + item['image_id'] + '.jpg'
# 但是有个问题, 图片会产生两份, 一份是hash命名的, 一份是自定义命名的
# 我没有深究下去, 用了os的方法
```

# 命令

```
//快速创建爬虫文件
scrapy genspider DemoSpider www.baidu.com

//爬取网页, 进入shell交互模式
//可以来测试selector
scrapy shell "www.baidu.com"

//列出所有爬虫
scrapy list
```

# setting.py文件

```python

# -*- coding: utf-8 -*-

# Scrapy settings for demo1 project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     http://doc.scrapy.org/en/latest/topics/settings.html
#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html

# Scrapy项目的名字, 这将用来构造默认 User-Agent, 同时也用来log, 当您使用 startproject 命令创建项目时其也被自动赋值
BOT_NAME = 'demo1'

# Scrapy搜索spider的模块列表 默认: [xxx.spiders]
SPIDER_MODULES = ['demo1.spiders']

# 使用 genspider 命令创建新spider的模块。默认: 'xxx.spiders'
NEWSPIDER_MODULE = 'demo1.spiders'


# 爬取的默认User-Agent，除非被覆盖
# USER_AGENT = 'demo1 (+http://www.yourdomain.com)'

# 如果启用, Scrapy将会采用 robots.txt策略
ROBOTSTXT_OBEY = True

# Scrapy downloader 并发请求(concurrent requests)的最大值, 默认: 16
# CONCURRENT_REQUESTS = 32

# 为同一网站的请求配置延迟(默认值：0)
# 下载器在下载同一个网站下一个页面前需要等待的时间,该选项可以用来限制爬取速度,减轻服务器压力。同时也支持小数:0.25 以秒为单位
# DOWNLOAD_DELAY = 3
    
# 下载延迟设置只有一个有效
# 对单个网站进行并发请求的最大值
# CONCURRENT_REQUESTS_PER_DOMAIN = 16
#对单个IP进行并发请求的最大值。如果非0,则忽略 CONCURRENT_REQUESTS_PER_DOMAIN 设定, 使用该设定。 也就是说, 并发限制将针对IP, 而不是网站。该设定也影响 DOWNLOAD_DELAY: 如果 CONCURRENT_REQUESTS_PER_IP 非0, 下载延迟应用在IP而不是网站上
# CONCURRENT_REQUESTS_PER_IP = 16

# 禁用Cookie(默认情况下启用)
# COOKIES_ENABLED = False

# 禁用Telnet控制台(默认启用)
# TELNETCONSOLE_ENABLED = False 

# 覆盖默认请求标头
# DEFAULT_REQUEST_HEADERS = {
#    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#    'Accept-Language': 'en',
# }

# 启用或禁用蜘蛛中间件
# SPIDER_MIDDLEWARES = {
#     'demo1.middlewares.Demo1SpiderMiddleware': 543,
# }

# 启用或禁用下载器中间件
# DOWNLOADER_MIDDLEWARES = {
#     'demo1.middlewares.MyCustomDownloaderMiddleware': 543,
# }

# 启用或禁用扩展程序
# EXTENSIONS = {
#     'scrapy.extensions.telnet.TelnetConsole': None,
# }

# 配置项目管道
# ITEM_PIPELINES = {
#     'demo1.pipelines.Demo1Pipeline': 300,
# }

# 启用和配置AutoThrottle扩展(默认情况下禁用)
# AUTOTHROTTLE_ENABLED = True

# 初始下载延迟
# AUTOTHROTTLE_START_DELAY = 5

# 在高延迟的情况下设置的最大下载延迟
# AUTOTHROTTLE_MAX_DELAY = 60


# Scrapy请求的平均数量应该并行发送每个远程服务器
# AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0

# 启用显示所收到的每个响应的调节统计信息：
# AUTOTHROTTLE_DEBUG = False

# 启用和配置HTTP缓存(默认情况下禁用)
# HTTPCACHE_ENABLED = True
# HTTPCACHE_EXPIRATION_SECS = 0
# HTTPCACHE_DIR = 'httpcache'
# HTTPCACHE_IGNORE_HTTP_CODES = []
# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
```

# 参考资料

[图片管道使用](https://www.cnblogs.com/Garvey/p/6691753.html)

[os.rename自定义图片名](https://segmentfault.com/q/1010000000413334)