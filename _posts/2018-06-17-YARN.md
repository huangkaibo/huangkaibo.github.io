---
title: YARN
date: 2018-06-17
tags: [大数据,Hadoop]
---

内容: 系统的介绍了YARN的架构,调度流程,以及和MapReduce1的对比

精华: 全部手打,把我不懂得问题都解释了一遍,图片也是精心绘制

<!-- more -->

# 概述

YARN
    : Yet Antother Recourse Negotiator/统一资源管理和调度平台

Hadoop1中MapReduce的调度是JobTracker/TaskTracker来进行的, 到了Hadoop2换成了YARN这个调度框架

## MapReduce1不足
    
Yarn的出现源于MapReduce1的不足:

* 可靠性差: MR1是主从架构, 主节点JobTracker挂掉整个集群就挂了
* 扩展性差: JobTracker承担资源管理和作业调度, 作业太多JobTracker就会成为集群瓶颈
* 资源利用率低: MapReduce1以map/reduce数目表示资源数目, 比如A节点资源数目为4个map和2个reduce, 这样划分非常不合理, 比如用了4个map, 还剩很多内存, 却只能放reduce而不能放map(详见[map槽与reduce槽](https://blog.csdn.net/ruidongliu/article/details/11689813))
* 无法支持异构的计算框架: MapReduce1是用于离线批处理, 但是还有其他计算需求, 如流处理/大规模并行处理, MapReduce1不支持欧中计算框架并存(这点不理解@TODO)

## 统一资源调度平台种类

### 集中式调度器

全局一个中央调度器, 所有资源请求全都交给中央调度器, 所有调度分配也由中央调度器完成, 故而高并发下中央调度器会成为瓶颈

如MapReduce1

### 双层调度器

调度工作分两层:  中央调度器, 框架调度器

中央调度器拥有集群全部资源信息, 按策略将资源分配给框架调度器

框架调度器无法看到全局, 只能看到所分配到的资源, 框架调度器将资源再分配给各容器执行具体计算任务

如Apache YARN和Apache Mesos

简单的说, 就是中心节点只负责大体的资源分配, 细节资源分配交给二级节点, 以此减压

### 状态共享调度器

还是分为: 中央调度器, 框架调度器

但是框架调度器会不断从中央调度器获取当前全局资源, 然后根据资源状态自己做出决策, 将决策同步给中央调度器(各框架调度器会竞争资源)

### 参考资料

[调度器种类详见](https://blog.csdn.net/u013080251/article/details/56278563)

# YARN架构

## 架构及流程

![](http://p1rbtn7qp.bkt.clouddn.com/18-5-31/15351974.jpg?imageView2/0/q/75|watermark/2/text/aHVhbmdrYWliby5naXRodWIuaW8=/font/5b6u6L2v6ZuF6buR/fontsize/320/fill/IzAwMDAwMA==/dissolve/100/gravity/SouthEast/dx/10/dy/10)

1. Client提交任务(提交ApplicationMaster运行程序和Job运行程序, 应该只是描述信息)给ResourceManager
2. ResourceManager返回HDFS地址
3. Client将ApplicationMaster运行程序和Job运行程序提交到HDFS
4. Client告知ResourceManager程序资源已上传完成
5. ResourceManager指定一个NodeManager让它拿出一个Container资源来运行ApplicationMaster
6. 这个NodeManager从HDFS下载ApplicationMaster运行程序并执行, 产生ApplicationMaster
7. ApplicationMaster向ResourceManager请求Job运行资源
8. ResourceManager返回分配了哪些NodeManager的哪些Container给它运行Job
9. ApplicationMaster联系这些NodeManager
10. 这些NodeManager从HDFS里下载Job程序资源
10. 这些NodeManager将所指定的Container拿来运行Job
11. 运行Job的Container定时将状态上报ApplicationMaster(供其监控处置)(是Container上报不是NodeManager上报)
12. 任务完成后ApplicationMaster向ResourceManager注销并关闭自己

## ResourceManager

### ResourceManager与JobTracker对比

同样是中心调度, 但是不同于JobTracker

JobTracker负责

* 调度: 资源分配
* 监控: 监控运行Job情况
* 管理: 重启各个失败任务

---

ResourceManager可以说只负责资源分配(且是一级资源分配), 将某节点(NodeManager)的某资源(Container)分配给你(ApplicationMaster)

而不负责Job监控, 也不负责这些资源的初始化(Job监控管理由ApplicationMaster负责, 资源初始化由NodeManager负责)

### ResourceManager作用

做了以下几件事: 

* 处理Client请求
* 接收NodeManager资源汇报信息, 以此了解集群资源分布状况
* 向NodeManager下令用一个Container开启ApplicationMaster
* 接受ApplicationMaster资源申请请求, 并为之分配资源
* 管理/失败重启ApplicationMaster

### ResourceManager结构

ResourceManager分为了两部分: Schedule/ApplicationManager

#### Schedule

Schedule: 纯调度器, 决定将哪个资源分配给哪个任务

#### ApplicationManager

应用程序管理器, 管理ApplicationMaster

1. 接受Client发来的Job请求
2. 向Schedule请求资源以启动ApplicationMaster
3. 监控/管理ApplicationMaster

## NodeManager

管理单个节点上的所有计算资源(计算资源就是Container)(不管理Job, NodeManager不知道自己节点上运行了什么Job)

* 听从ResourceManager和ApplicationMaster的指令启停一个Container(自己无法主动启停, 只能听命令)
* 向ResourceManager报告自身状态, ResourceManager由此了解集群资源分布情况
* 从HDFS中下载相关文件

每个节点一个NodeManager

### 参考资料

[NodeManager详细架构](https://www.cnblogs.com/yangykaifa/p/7015598.html)

## ApplicationMaster

分配二级资源来运行Job, 监控管理Job

向被分配的NodeManager联系告知Job, 指定哪个NodeManager运行Mapper哪个运行Reducer, 同时监控运行的Mapper/Reducer并管理

---

不同于MapReduce1监控管理Job是JobTracker来做的, JobTracker压力很大

MapReduce2里Job监控管理是Job开始前由ResourceManager指定一个节点来做为ApplicationMaster, 由它来负责监控管理Job

也就是监控管理Job任务分离出去了, 同时分布式了, 每一个Job都有一个ApplicationMaster, 压力变得很小

如果Job运行完, ApplicationMaster是会被销毁的, 因为ApplicationMaster也是占用一个Container资源的

### 可定制性

ApplicationMaster程序是用户上传的, 因此有可定制性, YARN以此来支持异构的计算框架

YARN是一个调度框架, 与MapReduce这个计算框架是完全解耦的, 因此Storm/Spark之类的计算框架也可以运行在YARN上

## Container

表示资源

MapReduce1中资源数目以slot定, 很粗糙不合理

现在资源用Container表示, Container里划分了内存/CPU等不同资源, 即提供隔离(以便未来继续加入网络等资源)

计算单元, 具体执行map或者reduce的基本单位

### Container结构

Container结构体如下, 可见指向了一处资源

```java
message ContainerProto {
    optional ContainerIdProto id = blabla; //container id
    optional NodeIdProto nodeId = blabla; //container（资源）所在节点
    optional string node_http_address = blabla;
    optional ResourceProto resource = blabla; //container资源量
    optional PriorityProto priority = blabla; //container优先级
    optional hadoop.common.TokenProto container_token = blabla; //container token，用于安全认证
}
```

### 资源隔离原理

CPU的资源隔离使用了CGroup这个技术

内存的资源隔离不能用CGroup, 因为CGroup如果超过了指定内存限制就会杀死该Job

然而java创建子进程采用fork()+exec()方式, fork()时内存占用与父进程一致, 因此可能超过所分配内存限制, 可能被CGroup杀死

所以采用了线程监控方案

#### 线程监控方案

赋予进程年龄属性, 刚启动的进程年龄是1, 监控没查看一次就加一

如果进程年龄>=1, 则内存超过设定值两倍就杀死(限制刚启动的, fork())

如果进程年龄>1, 则内存超过设定值就杀死(限制启动后的, fork()+exec()后的)

以此避免因为fork()被杀死

### 参考资料

[Container详细解释](https://blog.csdn.net/garfielder007/article/details/50936483)

# YARN/双层调度器

ResourceManager的资源分配只分配资源给ApplicationMaster, 只负责一级资源分配

ApplicationMaster负责与具体的Job运行节点沟通使用资源(虽然也是从ResourceManager获取这些资源, 但是ResourceManager给了就不管了, 主要还是ApplicationMaster来处理), 这是二级资源分配

双层调度, 以次减压

# YARN减压在哪里

原本JobTracker负责资源分配/监控/处理

现在ResourceManager只负责一级资源分配, 不监控

分布在各个节点的每个Job都有的ApplicationMaster负责二级资源分配处理, 监控处置Job

节点资源情况由NodeManager管理, 上报给ResourceManager

以此分工来减压

# YARN调度策略

即ResourceManager的Schedule的调度策略

## FIFO Schedule

所有Job按顺序排列, 一个执行完了再执行下一个

## Capacity Schedule

![](http://p1rbtn7qp.bkt.clouddn.com/18-5-31/86687301.jpg)

自定义队列树, 以及分配多少资源, 如上

default队列30%资源, analyst队列40%资源, dev队列30%资源, 
然后analyst再继续划分子树

在每个叶子队列里运行Job, 每个叶子队列内部以FIFO方式调度

当负荷最大时三个队列占比为3:4:3, 但是如果只有default队列被使用, 那么default队列资源量可以弹性增长超过30%(可以设置上限), 但是此时若analyst队列开始使用了, 那么只能等default队列Job运行完释放了资源, analyst才能开始执行Job

### 参考资料

[YARN资源调度策略之Capacity Scheduler](https://www.cnblogs.com/bugsbunny/p/6773016.html)

## Fair Schedule

每个Job的资源量动态均分, 如果只有1个Job就100%, 此时来了一个Job就各自50%, 走了一个就又变为100%

# 参考资料

[把YARN类比操作系统](http://dongxicheng.org/mapreduce-nextgen/understand-hadoop-yarn-from-os-view/?yyue=a21bo.50862.201879)

[比较详细的流程](https://blog.csdn.net/beidiqiuren/article/details/51759539)

[流程](http://dongxicheng.org/mapreduce-nextgen/understand-hadoop-yarn-from-os-view/?yyue=a21bo.50862.201879)

[为什么YARN是双层调度器](https://blog.csdn.net/jarth/article/details/52803759)

[MapReduce1和2对比](http://www.aboutyun.com/forum.php?mod=viewthread&tid=20891&yyue=a21bo.50862.201879)

[组件详解, 很值得一看](https://blog.csdn.net/suifeng3051/article/details/49486927)

[架构](https://blog.csdn.net/liuwenbo0920/article/details/43304243)

[模块组件, 较详细](https://blog.csdn.net/zhongqi2513/article/details/78115395?locationNum=1&fps=1)

[YARN比起MapReduce1的进步点](http://www.aboutyun.com/thread-6785-1-1.html?yyue=a21bo.50862.201879)

[YARN调度策略](https://www.iteblog.com/archives/1536.html)

[有很多东西](http://www.aboutyun.com/thread-17338-1-1.html?yyue=a21bo.50862.201879)